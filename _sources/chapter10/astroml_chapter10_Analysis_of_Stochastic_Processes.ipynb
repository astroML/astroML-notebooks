{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6b1332",
   "metadata": {
    "id": "9FuC628wBtGP"
   },
   "source": [
    "# Analysis of Stochastic Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46038c84",
   "metadata": {
    "id": "B5Tzdb5TBtGY"
   },
   "source": [
    "## Introduction\n",
    "Stochastic variability includes behavior that is not predictable forever as in the periodic case, but\n",
    "unlike temporally localized events, variability is always there. Typically, the underlying physics is\n",
    "so complex that we cannot deterministically predict future values.  \n",
    "Despite their seemingly irregular behavior, stochastic processes can be quantified. In this notebook, we will mainly discuss **autocorrelation**, **autoregressive** and **damped random walk** models in analyzing stochastic processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077992c",
   "metadata": {
    "id": "0eQvC1jxBtGY"
   },
   "source": [
    "## Import functions\n",
    "In this notebook, we mainly use functions from *astroML.time_series* and from *astroML.fourier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cd4d7",
   "metadata": {
    "id": "f6PdbWVOBtGY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.time_series import generate_power_law\n",
    "from astroML.fourier import PSD_continuous\n",
    "\n",
    "from astroML.time_series import lomb_scargle, generate_damped_RW\n",
    "from astroML.time_series import ACF_scargle, ACF_EK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ed78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0053b3",
   "metadata": {
    "id": "5Myf-dt-BtGZ"
   },
   "source": [
    "## The autocorrelation and structure functions\n",
    "One of the main statistical tools for the analysis of stochastic variability is the **autocorrelation\n",
    "function**. It represents a specialized case of the correlation function of two functions, f(t) and g(t),\n",
    "scaled by their standard deviations, and defined at time lag $\\Delta t$ as\n",
    "\n",
    "$$CF(\\Delta t) = \\frac{lim_{T \\rightarrow \\infty} \\frac{1}{T} \\int_{(T)}f(t)g(t+ \\Delta t)dt}{\\sigma_f \\sigma_g}$$\n",
    "\n",
    "where $\\sigma_f$ and $\\sigma_g$ are standard deviations of f(t) and g(t), respectively. With this normalization, the\n",
    "correlation function is unity for $\\Delta t$ = 0 (without normalization by standard deviation, the above\n",
    "expression is equal to the covariance function). It is assumed that both f and g are statistically\n",
    "weakly stationary functions, which means that their mean and autocorrelation function (see below)\n",
    "do not depend on time. The correlation function yields information about the time delay\n",
    "between two processes. If one time series is produced from another one by simply shifting the time\n",
    "axis by tlag, their correlation function has a peak at $\\Delta t = t_{lag}$.  \n",
    "  \n",
    "With $f(t) = g(t) = y(t)$, the autocorrelation of y(t) defined at time lag $\\Delta t$ is\n",
    "\n",
    "$$ACF(\\Delta t) = \\frac{lim_{T \\rightarrow \\infty} \\frac{1}{T} \\int_{(T)}y(t)y(t+ \\Delta t)dt}{\\sigma_y^2}$$\n",
    "\n",
    "The autocorrelation function yields information about the variable timescales present in a process. \n",
    "When y values are uncorrelated (e.g., due to white noise without any signal), $ACF(\\Delta t) = 0$, except for $ACF(0) =1$. For processes that \"retain memory\" of previous states only for some characteristic\n",
    "time $\\tau$, the autocorrelation function vanishes for $\\Delta t \\gg \\tau$.   \n",
    "  \n",
    "The autocorrelation function and the PSD of function y(t) (i.e. $PSD(f) \\equiv |H(f)|^2 + |H(-f)|^2$) are Fourier pairs; this\n",
    "fact is known as the Wiener-Khinchin theorem and applies to stationary random processes. The\n",
    "former represents an analysis method in the time domain, and the latter in the frequency domain.\n",
    "  \n",
    "The **structure function** is another quantity closely related to the autocorrelation function,\n",
    "\n",
    "$$SF(\\Delta t) = SF_\\infty [1-ACF(\\Delta t)]^{\\frac{1}{2}}$$\n",
    "\n",
    "where $SF_\\infty$ is the standard deviation of the time series evaluated over an infinitely large time\n",
    "interval (or at least much longer than any characteristic timescale $\\tau$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06870eee",
   "metadata": {
    "id": "zJgXWJamBtGa"
   },
   "source": [
    "### Examples of stochastic processes: $1/f$ and $1/f^2$ processes\n",
    "For a given autocorrelation function or PSD, the corresponding time series can be generated using\n",
    "the algorithm described in [On generating power law noise](http://adsabs.harvard.edu/full/1995A%26A...300..707T). Essentially, the amplitude of the Fourier transform is given by the\n",
    "PSD, and phases are assigned randomly; the inverse Fourier transform then generates time series.  \n",
    "The connection between the PSD and the appearance of time series of two power-law PSDs: $1/f$ and $1/f^2$ is illustrated in figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031c1c1",
   "metadata": {
    "id": "MXyV_sXrBtGa"
   },
   "source": [
    "#### 1. Generate data for plotting\n",
    "Let us use 1024 data points, and let $\\Delta t = 0.01$ in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db4555",
   "metadata": {
    "id": "M2zp-YDmBtGa"
   },
   "outputs": [],
   "source": [
    "N = 1024\n",
    "dt = 0.01\n",
    "factor = 100\n",
    "\n",
    "t = dt * np.arange(N)\n",
    "random_state = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe46b9",
   "metadata": {
    "id": "I6e7DqsUBtGb"
   },
   "source": [
    "#### 2. Calculate PSD and compare results\n",
    "We use *generate_power_law* in *astroML_time_series* package to apply the algorithm [On generating power law noise](http://adsabs.harvard.edu/full/1995A%26A...300..707T) on our generated data.  \n",
    "In the plot below shows examples of stochastic time series generated from power-law PSDs.\n",
    "* Plots of generated data (top panels) and the corresponding PSD (bottom panels) are shown.\n",
    "* Results from $1/f$ power-law (left panels) and from $1/f^2$ (right panel) are shown.\n",
    "* Dashed lines indicate input PSD, and solid lines are determined from time series shown in the top panels. \n",
    "\n",
    "\n",
    "The PSD normalization is such that both cases have similar\n",
    "power at low frequencies. For this reason, the overall amplitudes (more precisely, the variance) of\n",
    "the two time series are similar. The power at high frequencies is much larger for the $1/f$ case, and\n",
    "this is why the corresponding time series has the appearance of noisy data.  \n",
    "The structure function for the $1/f$ process is constant, and proportional to $t^{1/2}$ for\n",
    "the $1/f^2$ process (remember that we defined structure function with a square root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3c0dd",
   "metadata": {
    "id": "b7_W6DtcBtGb",
    "outputId": "91bbbd4f-07a5-459a-c077-af8475698a69"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7.5))\n",
    "fig.subplots_adjust(wspace=0.05)\n",
    "\n",
    "for i, beta in enumerate([1.0, 2.0]):\n",
    "    # Generate the light curve and compute the PSD\n",
    "    x = factor * generate_power_law(N, dt, beta, random_state=random_state)\n",
    "    f, PSD = PSD_continuous(t, x)\n",
    "\n",
    "    # First axes: plot the time series\n",
    "    ax1 = fig.add_subplot(221 + i)\n",
    "    ax1.plot(t, x, '-k')\n",
    "\n",
    "    ax1.text(0.95, 0.05, r\"$P(f) \\propto f^{-%i}$\" % beta,\n",
    "             ha='right', va='bottom', transform=ax1.transAxes)\n",
    "\n",
    "    ax1.set_xlim(0, 10.24)\n",
    "    ax1.set_ylim(-1.5, 1.5)\n",
    "\n",
    "    ax1.set_xlabel(r'$t$')\n",
    "\n",
    "    # Second axes: plot the PSD\n",
    "    ax2 = fig.add_subplot(223 + i, xscale='log', yscale='log')\n",
    "    ax2.plot(f, PSD, '-k')\n",
    "    ax2.plot(f[1:], (factor * dt) ** 2 * (2 * np.pi * f[1:]) ** -beta, '--k')\n",
    "\n",
    "    ax2.set_xlim(1E-1, 60)\n",
    "    ax2.set_ylim(1E-6, 1E1)\n",
    "\n",
    "    ax2.set_xlabel(r'$f$')\n",
    "\n",
    "    if i == 1:\n",
    "        ax1.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax2.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    else:\n",
    "        ax1.set_ylabel(r'${\\rm counts}$')\n",
    "        ax2.set_ylabel(r'$PSD(f)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5e2d17",
   "metadata": {
    "id": "zD_ADj6yBtGd"
   },
   "source": [
    "## Autocorrelation and structure function for evenly and unevenly sampled data\n",
    "### Evenly sampled data\n",
    "In the case of **evenly sampled data**, with $t_i = (i-1)\\Delta t$, the autocorrelation function of a discretely sampled $y(t)$ is defined as\n",
    "\n",
    "$$ACF(j) = \\frac{\\sum^{N-j}_{i=1} [(y_i-\\bar{y})(y_{i+j}-\\bar{y})]}{\\sum^{N}_{i=1}(y_i-\\bar{y})^2}$$\n",
    "\n",
    "The uncertainty of the mean is \n",
    "\n",
    "$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{N}}[1+2 \\sum^N_{j=1}(1-\\frac{j}{N})ACF(j)]^{1/2}$$\n",
    "\n",
    "where $\\sigma$ is the homoscedastic measurement error.   \n",
    "When a time series has a nonvanishing ACF, the uncertainty of its mean is larger than for an uncorrelated data set, where $\\sigma_{\\bar{x}} = \\frac{s}{\\sqrt{N}}$.  \n",
    "When data are unevenly sampled, the ACF cannot be computed using equations above.\n",
    "  \n",
    "### Unevenly sampled data\n",
    "For the case of **unevenly sampled data**, Edelson and Krolik proposed the **\"discrete correlation function\" (DCF)** in an astronomical context (called the \"slot autocorrelation function\" in physics). See [The Discrete Correlation Function](https://ui.adsabs.harvard.edu/abs/1988ApJ...333..646E/abstract).  \n",
    "For discrete unevenly sampled data with homoscedastic errors, they defined a quantity\n",
    "\n",
    "$$UDCF_{ij} = \\frac{(y_i-\\bar{y})(g_j-\\bar{g})}{[(\\sigma_y^2-e_y^2)(\\sigma_g^2-e_g^2)^{1/2}]}$$\n",
    "  \n",
    "where $e_y$ and $e_g$ are homoscedastic measurement errors for time series y and g. The associated time\n",
    "lag is $\\Delta t_{ij} = ti - tj$. The discrete correlation function at time lag $\\Delta t$ is then computed by binning\n",
    "and averaging $UDCF_{ij}$ over M pairs of points for which $\\Delta t-\\delta t=2 \\leq \\delta t_{ij} \\leq \\Delta t+\\delta t/2$, where $\\delta t$ is\n",
    "the bin size. The bin size is a trade-of between accuracy of $DCF(\\Delta t)$ and its resolution. Edelson and Krolik showed that even uncorrelated time series will produce values of the cross-correlation $DCF(\\Delta t) \\sim \\pm 1/ \\sqrt{M}$.\n",
    "  \n",
    "Scargle has developed different techniques to evaluate the discrete Fourier transform, correlation\n",
    "function and autocorrelation function of unevenly sampled time series (see [Studies in astronomical time series analysis. III.](https://www.osti.gov/biblio/5344858-studies-astronomical-time-series-analysis-iii-fourier-transforms-autocorrelation-functions-cross-correlation-functions-unevenly-spaced-data)).  \n",
    "Given an unevenly sampled time series, y(t), the essential steps of Scargle's\n",
    "procedure are as follows:\n",
    "1. Compute the generalized Lomb-Scargle periodogram for $y(t_i), i = 1,..., N$, namely $P_{LS}(\\omega)$.\n",
    "2. Compute the sampling window function using the generalized Lomb-Scargle periodogram using $z(t_i) = 1, i = 1,..., N$, namely $P_{LS}^W(\\omega)$.\n",
    "3. Compute inverse Fourier transforms for $P_{LS}(\\omega)$ and $P^W_{LS}(\\omega)$, namely $\\rho(t)$ and $\\rho^W(t)$, respectively.\n",
    "4. The autocorrelation function at lag t is $ACF(t) = \\rho(t)/ \\rho^W(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed197bf2",
   "metadata": {
    "id": "_Uy3m54uBtGe"
   },
   "source": [
    "### Edelson and Krolik's DCF method and the Scargle method Demonstration\n",
    "We will see an example of the use of Edelson and Krolik's DCF method and the Scargle method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296a483",
   "metadata": {
    "id": "WTrvczHcBtGf"
   },
   "source": [
    "#### 1. Generate time-series data\n",
    "We use do 1000 days worth of magnitudes for this sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53fbded",
   "metadata": {
    "id": "0iNZR_vFBtGf"
   },
   "outputs": [],
   "source": [
    "t = np.arange(0, 1E3)\n",
    "z = 2.0\n",
    "tau = 300\n",
    "tau_obs = tau / (1. + z)\n",
    "\n",
    "np.random.seed(6)\n",
    "y = generate_damped_RW(t, tau=tau, z=z, xmean=20)\n",
    "\n",
    "# randomly sample 100 of these\n",
    "ind = np.arange(len(t))\n",
    "np.random.shuffle(ind)\n",
    "ind = ind[:100]\n",
    "ind.sort()\n",
    "t = t[ind]\n",
    "y = y[ind]\n",
    "\n",
    "# add errors\n",
    "dy = 0.1\n",
    "y_obs = np.random.normal(y, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6ca1a",
   "metadata": {
    "id": "dyipthPvBtGf"
   },
   "source": [
    "#### 2. Compute ACF via Scargle method\n",
    "We calculate the ACF following Scargle method process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e3c9d",
   "metadata": {
    "id": "Ig96m319BtGg",
    "outputId": "204c466d-befa-47c6-a6f7-9940e89dab95"
   },
   "outputs": [],
   "source": [
    "C_S, t_S = ACF_scargle(t, y_obs, dy,\n",
    "                       n_omega=2. ** 12, omega_max=np.pi / 5.0)\n",
    "\n",
    "ind = (t_S >= 0) & (t_S <= 500)\n",
    "t_S = t_S[ind]\n",
    "C_S = C_S[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e7234",
   "metadata": {
    "id": "lxiSrmh-BtGg"
   },
   "source": [
    "#### 3. Compute ACF via E-K method\n",
    "We calculate ACF using Edelson and Krolik's DCF method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf4cba",
   "metadata": {
    "id": "LVF4PNWTBtGg"
   },
   "outputs": [],
   "source": [
    "C_EK, C_EK_err, bins = ACF_EK(t, y_obs, dy, bins=np.linspace(0, 500, 51))\n",
    "t_EK = 0.5 * (bins[1:] + bins[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9f6db",
   "metadata": {
    "id": "8YaYEJWHBtGh"
   },
   "source": [
    "#### 4. Compare results\n",
    "Example of the autocorrelation function for a stochastic process is shown below. \n",
    "* The top panel shows a simulated light curve generated using a damped random walk model. \n",
    "* The bottom panel shows the corresponding autocorrelation function computed using Edelson and Krolik's DCF method and the Scargle method. \n",
    "* The solid line shows the input autocorrelation function used to generate the light curve.  \n",
    "\n",
    "The two methods produce similar results. Errors are easier to compute for the DCF method and this advantage\n",
    "is crucial when fitting models to the autocorrelation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb61408",
   "metadata": {
    "id": "CFlHUo3iBtGh",
    "outputId": "b97239f6-6154-46e7-c359-f34db66ea8af"
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot the input data\n",
    "ax = fig.add_subplot(211)\n",
    "ax.errorbar(t, y_obs, dy, fmt='.k', lw=1)\n",
    "ax.set_xlabel('t (days)')\n",
    "ax.set_ylabel('observed flux')\n",
    "\n",
    "# plot the ACF\n",
    "ax = fig.add_subplot(212)\n",
    "ax.plot(t_S, C_S, '-', c='gray', lw=1,\n",
    "        label='Scargle')\n",
    "ax.errorbar(t_EK, C_EK, C_EK_err, fmt='.k', lw=1,\n",
    "            label='Edelson-Krolik')\n",
    "ax.plot(t_S, np.exp(-abs(t_S) / tau_obs), '-k', label='True')\n",
    "ax.legend(loc=3)\n",
    "\n",
    "ax.plot(t_S, 0 * t_S, ':', lw=1, c='gray')\n",
    "\n",
    "ax.set_xlim(0, 500)\n",
    "ax.set_ylim(-1.0, 1.1)\n",
    "\n",
    "ax.set_xlabel('t (days)')\n",
    "ax.set_ylabel('ACF(t)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e301110",
   "metadata": {
    "id": "b8IOOHERBtGi"
   },
   "source": [
    "## Autoregressive models\n",
    "Autocorrelated time series can be analyzed and characterized using stochastic **\"autoregressive models.\"**\n",
    "Autoregressive models provide a good general description of processes that \"retain memory\"\n",
    "of previous states (but are not periodic). An example of such a model is the **random walk**, where\n",
    "each new value is obtained by adding noise to the preceding value:\n",
    "\n",
    "$$y_i = y_{i-1} + e_i$$\n",
    "\n",
    "When $y_{i-1}$ is multiplied by a constant factor greater than 1, the model is known as a geometric\n",
    "random walk model (used extensively to model stock market data). The noise need not be Gaussian.  \n",
    "  \n",
    "The random walk can be generalized to the **linear autoregressive (AR) model** with dependencies\n",
    "on k past values (i.e., not just one as in the case of random walk). An autoregressive process of\n",
    "order k, AR(k), for a discrete data set is defined by  \n",
    "\n",
    "$$y_i = \\sum^{k}_{j=1} a_j y_{i-j} + e_i$$\n",
    "\n",
    "That is, the latest value of y is expressed as a linear combination of the k previous values of y, with\n",
    "the addition of noise (for random walk, k = 1 and a1 = 1). If the data are drawn from a stationary\n",
    "process, coefficients aj satisfy certain conditions. The ACF for an AR(k) process is nonzero for all\n",
    "lags, but it decays quickly.  \n",
    "  \n",
    "An autoregressive process defined by $y_i = \\sum^{k}_{j=1} a_j y_{i-j} + e_i$ applies only to evenly sampled time series. A\n",
    "generalization is called the **continuous autoregressive process, CAR(k)**; see [Are the Variations in Quasar Optical Flux Driven by Thermal Fluctuations](https://ui.adsabs.harvard.edu/abs/2009ApJ...698..895K/abstract). The CAR(1)\n",
    "process has recently received a lot of attention in the context of quasar variability.\n",
    "\n",
    "In addition to autoregressive models, data can be modeled using the **covariance matrix** (e.g., using\n",
    "Gaussian process in Gaussian process regression section). For example, for the CAR(1) process,\n",
    "\n",
    "$$S_{ij} = \\sigma^2 exp(-|t_{ij} / \\tau|)$$\n",
    "\n",
    "where $\\sigma$ and $\\tau$ are model parameters; $\\sigma^2$ controls the short timescale covariance ($t_{ij} \\ll \\tau$),\n",
    "which decays exponentially on a timescale given by $\\tau$. A number of other convenient models\n",
    "and parametrizations for the covariance matrix are discussed in the context of quasar variability\n",
    "in [Is quasar variability a damped random walk?](https://arxiv.org/abs/1202.3783)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f96795c",
   "metadata": {
    "id": "zzb8NG1GBtGi"
   },
   "source": [
    "## Damped random walk model\n",
    "The CAR(1) process is described by a stochastic differential equation which includes a damping\n",
    "term that pushes y(t) back to its mean (see [Is quasar variability a damped random walk?](https://arxiv.org/abs/1202.3783)); \n",
    "\n",
    "hence, it is also known as damped random walk. In analogy with calling random walk \"drunkard's walk,\" damped random walk could\n",
    "be called \"married drunkard's walk\" (who always comes home instead of drifting away).\n",
    "Following $S_{ij} = \\sigma^2 exp(-|t_{ij} / \\tau|)$, the autocorrelation function for a damped random walk is\n",
    "\n",
    "$$ACF(t) = exp(-t/ \\tau)$$\n",
    "\n",
    "where $\\tau$ is the characteristic timescale (relaxation time, or damping timescale). Given the $ACF$,\n",
    "it is easy to show that the structure function is\n",
    "\n",
    "$$SF(t) = SF_{\\infty} [1-exp(-t/ \\tau)]^{1/2}$$\n",
    "\n",
    "where $SF_{\\infty}$ is the asymptotic value of the structure function equal to $\\sqrt{2} \\sigma$, where $\\sigma$ has the same definition as in $S_{ij} = \\sigma^2 exp(-|t_{ij} / \\tau|)$. When the structure function applies to differences of the analyzed process\n",
    "\n",
    "$$PSD(f) = \\frac{\\tau^2 SF^2_{\\infty}}{1+(2 \\pi f \\tau)^2}$$\n",
    "\n",
    "Therefore, the damped random walk is a $1/f^2$ process at high frequencies, just as ordinary random\n",
    "walk. The \"damped nature\" is seen as the at PSD at low frequencies ($f \\ll 2\\pi / \\tau$). In the previous figure demonstrating E-K method and Scargle method, a light curve generated using a damped random walk is shown in the top panel.  \n",
    "  \n",
    "For evenly sampled data, the $CAR(1)$ process is equivalent to the $AR(1)$ process with $a_1 =\n",
    "exp(-1 / \\tau)$, that is, the next value of $y$ is the damping factor times the previous value plus noise.\n",
    "The noise for the $AR(1)$ process, $\\sigma_{AR}$ is related to $SF_{\\infty}$ via\n",
    "\n",
    "$$\\sigma_{AR} = \\frac{SF_{\\infty}}{\\sqrt{2}} [1-exp(-2 / \\tau)]^{1/2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb014d19",
   "metadata": {
    "id": "0q9zlSEuBtGi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   15,
   20,
   28,
   33,
   45,
   50,
   86,
   94,
   99,
   110,
   127,
   169,
   205,
   210,
   215,
   239,
   244,
   256,
   261,
   268,
   279,
   310,
   348,
   378
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}