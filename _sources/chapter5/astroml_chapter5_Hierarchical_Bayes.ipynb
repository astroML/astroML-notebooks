{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a163523",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hierarchical Bayes Example\n",
    "### January 6, 2020 \n",
    "\n",
    "\n",
    "[astroML workshop at the 235th Meeting of the American Astronomical Society](http://www.astroml.org/workshops/AAS235.html)\n",
    "\n",
    "[Zeljko Ivezic, University of Washington](http://faculty.washington.edu/ivezic/) \n",
    "\n",
    "[This notebook](https://github.com/astroML/astroML-workshop_AAS235/blob/master/bayesian/AAS2019_HBexample.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbc829",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Resources for this notebook include:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 5.  \n",
    "- [Jake VanderPlas' workshop \"Bayesian Astronomy\"](https://github.com/jakevdp/BayesianAstronomy)\n",
    "- [Jake VanderPlas' blog \"Frequentism and Bayesianism: A Practical Introduction\"](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5742f1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Highly recommended supplemental background reading:\n",
    "\n",
    "- [Mario Juric: Frequentist vs. Bayesian Perspectives (LSSTC Data Science Fellowship Program Lecture)](https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/tree/master/Session4/Day1)\n",
    "- [Jake VanderPlas: ``Frequentism and Bayesianism: A Python-driven Primer\"](https://arxiv.org/abs/1411.5018)\n",
    "- [Hogg, Bovy and Lang: ``Data analysis recipes: Fitting a model to data\"](https://arxiv.org/abs/1008.4686)\n",
    "\n",
    "\n",
    "##### For those who want to dive deep:\n",
    "\n",
    "- [D. Sivia and J. Skilling: ``Data Analysis: A Bayesian Tutorial''](https://www.amazon.com/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320)\n",
    "- [E.T. Jaynes: ``Probability Theory: The Logic of Science''](http://bayes.wustl.edu/etj/prob/book.pdf)\n",
    "- [E.T. Jaynes: ``Confidence Intervals vs. Bayesian intervals''](http://bayes.wustl.edu/etj/articles/confidence.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ab74b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Bayes Modeling\n",
    "\n",
    "(a.k.a. multilevel models, Bayesian belief networks, or graphical models)\n",
    "\n",
    "\n",
    "We'll start with a quick reminder of Bayes' Theorem and its implications for today's discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2fe50b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Bayesian Statistics](figures/bayes2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1e1e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple case of Gaussian prior and likelihood  \n",
    "\n",
    ">  ***Bayes' Theorem:*** $$p(M|D) = \\frac{p(D|M)p(M)}{p(D)} $$\n",
    "  \n",
    "Let's assume a one-dimensional problem: we measure length (or mass, or velocity, or metallicity, or ...) $x$, with measurement uncertainty $e$, and we are trying to estimate $\\mu$, the true length. \n",
    "\n",
    "We have prior information (belief) for the value of $\\mu$ that can be summarized as N($\\mu_0$, $\\sigma_0$). For example, this could be the result of a number of older (prior) measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b058363f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple case of Gaussian prior and likelihood  \n",
    "\n",
    ">  ***Bayes' Theorem:*** $$p(M|D) = \\frac{p(D|M)p(M)}{p(D)} $$\n",
    "\n",
    "\n",
    "\n",
    "The posterior probability density function is then \n",
    "\n",
    "$$ p(\\mu | x, e, \\mu_0, \\sigma_0) \\propto \\frac{1}{e\\sqrt{2\\pi}} \\exp\\left(\\frac{-(\\mu - x)^2}{2e^2}\\right) \\times$$\n",
    "$$ \\frac{1}{\\sigma_0\\sqrt{2\\pi}} \\exp\\left(\\frac{-(\\mu - \\mu_0)^2}{2 \\sigma_0^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b3ba6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When the posterior probability density function is a product of two Gaussians, \n",
    "it is easy to show that $ p(\\mu | x, e, \\mu_0, \\sigma_0)$ can be summarized as a Gaussian\n",
    "$$ p(\\mu | x, e, \\mu_0, \\sigma_0) = \\frac{1}{\\sigma_p\\sqrt{2\\pi}} \\exp\\left(\\frac{-(\\mu - \\mu_p)^2}{2\\sigma_p^2}\\right),$$\n",
    "\n",
    "with \n",
    "$$\\mu_p = \\frac{(x/e^2) + (\\mu_0/\\sigma_0^2)}{1/e^2 + 1/\\sigma_0^2}  $$\n",
    "and \n",
    "$$ \\sigma_p = \\left( 1/e^2 + 1/\\sigma_0^2 \\right)^{-1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2f7bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple case of Gaussian prior and likelihood  \n",
    "\n",
    "This result, where $\\mu_p$ is obtained by inverse variance weighting, \n",
    "$$\\mu_p = \\frac{(x/e^2) + (\\mu_0/\\sigma_0^2)}{1/e^2 + 1/\\sigma_0^2} \\,\\,\\,\\, {\\rm and} \\,\\,\\,\\, \\sigma_p = \\left( 1/e^2 + 1/\\sigma_0^2 \\right)^{-1/2}$$\n",
    "is quite simple, but very educational. Let's plot an example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab8449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8601942",
   "metadata": {},
   "outputs": [],
   "source": [
    "## case of conjugate prior: both prior and posterior pdfs are gaussian\n",
    "x = 0.7 \n",
    "e = 0.2\n",
    "mu0 = 1.4\n",
    "sigma0 = 0.3 \n",
    "muGrid = np.linspace(0,3,1000)\n",
    "n1 = norm(x, e).pdf(muGrid) \n",
    "n2 = norm(mu0, sigma0).pdf(muGrid) \n",
    "P = n1 * n2 \n",
    "# need to properly normalize P to make it pdf \n",
    "from scipy.integrate import simps\n",
    "P = P/simps(P, muGrid)\n",
    "\n",
    "# for fun, compute mu_p and sigma_p and compare to direct multiplication of n1 and n2\n",
    "sigma_p = 1/np.sqrt(1/e**2+1/sigma0**2)\n",
    "mu_p = (x/e**2 + mu0/sigma0**2)*sigma_p**2\n",
    "posterior = norm(mu_p, sigma_p).pdf(muGrid) \n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "plt.plot(muGrid, n1, ls='-', c='green', label=r'measurement')\n",
    "plt.plot(muGrid, n2, ls='-', c='black', label=r'prior')\n",
    "plt.plot(muGrid, P, ls='-', c='blue', label=r'direct product')\n",
    "plt.plot(muGrid, posterior, ls='--', c='red', label=r'posterior')\n",
    "\n",
    "\n",
    "plt.xlim(0.0, 2.5)\n",
    "plt.ylim(0, 3.0)\n",
    "plt.xlabel('$\\mu$')\n",
    "plt.ylabel(r'$p(x_i|\\mu,\\sigma)$')\n",
    "plt.title('Gaussian prior and Gaussian Likelihood')\n",
    "plt.legend()\n",
    "plt.show() \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296c5c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The posterior pdf is ***narrower*** than both measurement pdf (i.e. likelihood) and prior pdf (recall that measurement precision often scales as $1/\\sqrt{N}$, where $N$ is the number of data points)\n",
    "\n",
    "- The posterior pdf is ***biased*** compared to the maximum likelihood estimate! \n",
    "\n",
    "- In this example, the likelihood and prior pdfs can be switched and the posterior pdf will remain unchanged\n",
    "\n",
    "- The same posterior pdf is the solution when prior is based on $N-1$ data points and likelihood corresponds to the $N$th data point: an online algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba7423c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's now assume we have N measurements (data points)\n",
    "\n",
    "### Estimating parameters of a Gaussian distribution using data with homoscedastic Gaussian uncertainties\n",
    "\n",
    "The data likelihood can be written as:\n",
    "\n",
    "$$  p(\\{x_i\\}|\\mu,\\sigma,I) = \\prod_{i=1}^N  {1 \\over \\sqrt{2\\pi} (\\sigma^2+e^2)^{1/2}}\n",
    "                   \\exp{\\left({-(x_i-\\mu)^2 \\over 2 (\\sigma^2+e^2)}\\right)}$$\n",
    "                   \n",
    "Here, $\\mu$ and $\\sigma$ can still describe the prior pdf (after rearragning to posterior pdf\n",
    "via Bayes' theorem). But we can also interpret $\\mu$ and $\\sigma$ as describing an additional \n",
    "(part of the) process that generates data, and that are themselves drawn from flat priors - math\n",
    "is exactly the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b37c5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let's now assume we have N measurements (data points)\n",
    "\n",
    "### Estimating parameters of a Gaussian distribution using data with homoscedastic Gaussian uncertainties\n",
    "\n",
    "The posterior pdf for $\\mu$ and $\\sigma$ is: \n",
    "\n",
    "$$  p(\\mu,\\sigma | \\{x_i\\},I) = \\prod_{i=1}^N  {1 \\over \\sqrt{2\\pi} (\\sigma^2+e^2)^{1/2}}\n",
    "                   \\exp{\\left({-(x_i-\\mu)^2 \\over 2 (\\sigma^2+e^2)}\\right)}$$\n",
    "                   \n",
    "Because errors are homoscedastic (all data points have uncertainty $e$), there is a\n",
    "closed-form solution. For example, we can estimate $\\sigma^2$ by subtracting $e^2$ from\n",
    "the variance of $\\{x_i\\}$ (i.e. if $\\sigma=0$, the width of $\\{x_i\\}$ distribution would be simply $e$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a7cf9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Here is another relatively simple case to bring us closer to the main idea of Hierarchical Bayes: \n",
    "\n",
    "### Estimating parameters of a Gaussian distribution using data with heteroscedastic Gaussian uncertainties\n",
    "\n",
    "The posterior pdf for $\\mu$ and $\\sigma$ is:  \n",
    "$$  p(\\mu,\\sigma | \\{x_i\\},I) = \\prod_{i=1}^N  {1 \\over \\sqrt{2\\pi} (\\sigma^2+e_i^2)^{1/2}}\n",
    "                   \\exp{\\left({-(x_i-\\mu)^2 \\over 2 (\\sigma^2+e_i^2)}\\right)}$$\n",
    "\n",
    "There is no closed-form solution here because errors are heteroscedastic (each data point\n",
    "has a different uncertainty $e_i$), but we can easily evaluate posterior pdf (say, with uniform priors)\n",
    "on a grid of $\\mu$ and $\\sigma$ (or use more sophisticated methods such as MCMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927e982",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Estimating parameters of a Gaussian distribution using data with heteroscedastic Gaussian uncertainties\n",
    "\n",
    "The posterior pdf for $\\mu$ and $\\sigma$ is:  \n",
    "$$  p(\\mu,\\sigma | \\{x_i\\},I) = \\prod_{i=1}^N  {1 \\over \\sqrt{2\\pi} (\\sigma^2+e_i^2)^{1/2}}\n",
    "                   \\exp{\\left({-(x_i-\\mu)^2 \\over 2 (\\sigma^2+e_i^2)}\\right)}$$\n",
    "\n",
    "There is no closed-form solution here because errors are heteroscedastic (each data point\n",
    "has a different uncertainty $e_i$), but we can easily evaluate posterior pdf (say, with uniform priors)\n",
    "on a grid of $\\mu$ and $\\sigma$ (or use more sophisticated methods such as MCMC). \n",
    "\n",
    "The key point is that the scatter of $\\{x_i\\}$ is larger than it would be for $\\sigma=0$ (that is, $\\chi^2>1$)\n",
    "and thus it constrains $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49db2ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An astrophysical example: radial velocity dispersion for a cluster\n",
    "\n",
    "Consider measurements of the radial velocities of $N$ stars from a stellar or\n",
    "galaxy cluster. \n",
    "\n",
    "Assume that the cluster radial velocity distribution can be \n",
    "fully characterized by its mean, the systemic velocity (because the whole \n",
    "cluster is moving relative to the observer), and its standard deviation, \n",
    "the cluster velocity dispersion (because objects are gravitationally tied to \n",
    "the cluster and have orbital motions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87920d60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An astrophysical example: radial velocity dispersion for a cluster\n",
    "\n",
    "Consider measurements of the radial velocities of $N$ stars from a stellar or\n",
    "galaxy cluster. \n",
    "\n",
    "The observed dispersion (i.e. standard deviation) of measured velocities \n",
    "is always larger than the cluster velocity dispersion because of measurement\n",
    "errors.  \n",
    "\n",
    "When measurement errors are homoscedastic, we can estimate the cluster velocity \n",
    "dispersion, by subtracting the measurement error contribution from the observed\n",
    "dispersion (i.e. standard deviation) of measured velocities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2799ad41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### An astrophysical example: radial velocity dispersion for a cluster\n",
    "\n",
    "When errors are heteroscedastic, we don't have a closed-form solution.\n",
    "\n",
    "We can assume that the true radial velocity of a single object is drawn\n",
    "from a Gaussian described by $\\mu$ (systemic velocity) and $\\sigma$ (velocity\n",
    "dispersion). But we don't know them and thus they need to be estimated from data! \n",
    "\n",
    "In case of many objects, they will collectively constrain $\\mu$ and $\\sigma$.\n",
    "Many independent measurement sets (here a set of one measurement per object) \n",
    "that share same priors constrain them better together than can any single \n",
    "measurement alone. This is the key idea of Hierarchical Bayes modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ed514",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical Bayes \n",
    "\n",
    "In hierarchical, or multilevel, Bayesian analysis a prior distribution depends \n",
    "on unknown variables, the hyperparameters, that describe the group (population) \n",
    "level probabilistic model. \n",
    "\n",
    "Their priors, called hyperpriors, resemble the priors in simple (single-level) \n",
    "Bayesian models. \n",
    "\n",
    "In the radial velocity example above, $\\mu$ and $\\sigma$ are priors, and their \n",
    "corresponding prior distributions (assumed flat below), are hyperpriors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0846d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical Bayes \n",
    "\n",
    "In hierarchical, or multilevel, Bayesian analysis a prior distribution depends \n",
    "on unknown variables, the hyperparameters, that describe the group (population) \n",
    "level probabilistic model. \n",
    "\n",
    "When there are many independent measurement sets that share same priors, they together constrain priors better than can any single measurement alone. \n",
    "\n",
    "In statistics, this effect is known as borrowing strength and is related to \n",
    "the concept of shrinkage estimators. \n",
    "\n",
    "Let's now look at our radial velocity example numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557bad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.plotting.mcmc import convert_to_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussgauss_logL(xi, ei, mu, sigma):\n",
    "    \"\"\"Equation 5.63: gaussian likelihood with gaussian errors\"\"\"\n",
    "    ndim = len(np.broadcast(sigma, mu).shape)\n",
    "\n",
    "    xi = xi.reshape(xi.shape + tuple(ndim * [1]))\n",
    "    ei = ei.reshape(ei.shape + tuple(ndim * [1]))\n",
    "\n",
    "    s2_e2 = sigma ** 2 + ei ** 2\n",
    "    return -0.5 * np.sum(np.log(s2_e2) + (xi - mu) ** 2 / s2_e2, 0)\n",
    "\n",
    "def getExpStD(x, p):\n",
    "    \"\"\"given p(x), compute expectation value and std. dev.\"\"\"\n",
    "    Ex = np.sum(x * p) / np.sum(p)\n",
    "    Sx = np.sqrt(np.sum((x - Ex) ** 2 * p) /  np.sum(p))\n",
    "    return Ex, Sx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "np.random.seed(2)    # for repeatability\n",
    "\n",
    "N = 10               # number of measured stars\n",
    "mu_true = -50.0      # km/s, true systemic velocity\n",
    "sigma_true = 20.0    # km/s, true velocity dispersion\n",
    "ei = 10 + 40 * np.random.random(N)   # n.b. heteroscedastic errors\n",
    "# generate measurements\n",
    "xi = np.random.normal(mu_true, np.sqrt(sigma_true ** 2 + ei ** 2))\n",
    "wi = 1 / ei ** 2 / np.sum(1 / ei ** 2)\n",
    "# weighted mean\n",
    "wmean = np.sum(wi * xi)\n",
    "# uncertainty of weighted mean\n",
    "wmeane = 1 / np.sqrt(np.sum(1 / ei ** 2))\n",
    "# other stats\n",
    "medvel = np.median(xi)\n",
    "meanvel = np.mean(xi)\n",
    "velstd = np.std(xi)\n",
    "\n",
    "# define the grids and compute logL\n",
    "sigma = np.linspace(0.01, 120, 70)\n",
    "mu = np.linspace(-150, 50, 70)\n",
    "\n",
    "logL = gaussgauss_logL(xi, ei, mu, sigma[:, np.newaxis])\n",
    "logL -= logL.max()\n",
    "L = np.exp(logL)\n",
    "\n",
    "# integrate L to get marginal prob. distributions\n",
    "p_sigma = L.sum(1)\n",
    "p_sigma /= (sigma[1] - sigma[0]) * p_sigma.sum()\n",
    "\n",
    "p_mu = L.sum(0)\n",
    "p_mu /= (mu[1] - mu[0]) * p_mu.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------\n",
    "from astroML.plotting import setup_text_plots\n",
    "setup_text_plots(fontsize=16, usetex=False)\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure(figsize=(12, 9.0))\n",
    "\n",
    "fig.subplots_adjust(left=0.1, right=0.95, wspace=0.24,\n",
    "                    bottom=0.15, top=0.9)\n",
    "\n",
    "fig.add_axes((0.58, 0.55, 0.30, 0.40))\n",
    "\n",
    "plt.imshow(logL, origin='lower',\n",
    "           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
    "           cmap=plt.cm.binary,\n",
    "           aspect='auto')\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)\n",
    "\n",
    "plt.contour(mu, sigma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),\n",
    "            colors='k')\n",
    "\n",
    "plt.xlabel(r'${\\rm systemic \\, velocity \\, } v_s \\, {\\rm (km/s)}$')\n",
    "plt.ylabel(r'${\\rm intrinsic \\, vel. \\, dispersion \\,} \\sigma \\, {\\rm (km/s)}$')\n",
    "plt.xlim(-150, 50.0)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# plot true values\n",
    "plt.plot([mu_true, mu_true], [0, 100.0], ':r', lw=1)\n",
    "plt.plot([-200, 200.0], [sigma_true, sigma_true], ':r', lw=1)\n",
    "\n",
    "# second axis: marginalized over mu\n",
    "ax2 = fig.add_axes((0.17, 0.1, 0.3, 0.30))\n",
    "ax2.plot(mu, p_mu, '-k', label='')\n",
    "ax2.set_xlabel(r'$v_s$ (km/s)')\n",
    "ax2.set_ylabel(r'$p(v_s)$')\n",
    "ax2.set_xlim(-100, 0.0)\n",
    "ax2.set_ylim(0, 0.04)\n",
    "# mark expectation value for radial velocity\n",
    "Ev, Sv = getExpStD(mu, p_mu)\n",
    "plt.plot([Ev, Ev], [0, 100.0], 'g', lw=1)\n",
    "# mark true systemic velocity and weighted mean of data\n",
    "plt.plot([mu_true, mu_true], [0, 100.0], ':r', lw=1)\n",
    "plt.plot([wmean, wmean], [0, 100.0], '--b', lw=1)\n",
    "\n",
    "# plot the marginalized distribution for sigma\n",
    "ax3 = fig.add_axes((0.58, 0.1, 0.3, 0.30))\n",
    "ax3.plot(sigma, p_sigma, '-k', label='')\n",
    "ax3.set_xlabel(r'$\\sigma$ (km/s)')\n",
    "ax3.set_ylabel(r'$p(\\sigma)$')\n",
    "ax3.set_xlim(0, 100.0)\n",
    "ax3.set_ylim(0, 0.05)\n",
    "plt.plot([sigma_true, sigma_true], [0, 100.0], ':r', lw=1)\n",
    "Ed, Sd = getExpStD(sigma, p_sigma)\n",
    "plt.plot([Ed, Ed], [0, 100.0], 'g', lw=1)\n",
    "\n",
    "# plot data\n",
    "ax4 = fig.add_axes((0.17, 0.55, 0.3, 0.40))\n",
    "ax4.set_xlabel(r'$v_{obs}$ (km/s)')\n",
    "ax4.set_ylabel(r'measurement index')\n",
    "ax4.set_xlim(-150, 50)\n",
    "ax4.set_ylim(0, 11)\n",
    "# mark +-error ranges\n",
    "for i in range(0, N):\n",
    "    xL = xi[i] - ei[i]\n",
    "    xR = xi[i] + ei[i]\n",
    "    plt.plot([xL, xR], [i + 1, i + 1], 'b', lw=2)\n",
    "# mark true systemic velocity and weighted mean of data\n",
    "plt.plot([wmean, wmean], [0, 100.0], '--b', lw=1)\n",
    "plt.plot([mu_true, mu_true], [0, 100.0], ':r', lw=1)\n",
    "\n",
    "# mark posterior range for each star\n",
    "mup = Ev\n",
    "sigp = Ed\n",
    "for i in range(0, N):\n",
    "    sig0 = 1 / np.sqrt(1 / sigp ** 2 + 1 / ei[i] ** 2)\n",
    "    mu0 = (mup / sigp ** 2 + xi[i] / ei[i] ** 2) * (sig0 ** 2)\n",
    "    xL = mu0 - sig0\n",
    "    xR = mu0 + sig0\n",
    "    plt.plot([xL, xR], [i + 0.7, i + 0.7], 'g', lw=1)\n",
    "\n",
    "# and expectation value for systemic velocity\n",
    "plt.plot([mup, mup], [0, 100.0], 'g', lw=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c6b8d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "In this particular example, the cluster velocity dispersion is astrophysically the most interesting quantity, while the posterior constraints on radial velocities for individual stars are less important. \n",
    "\n",
    "However, in other settings described by the same mathematical model the posterior pdfs for the individually measured quantities (i.e., the true radial velocity for each star in the above example) may be of more interest than the priors. In such cases the posterior pdf is marginalized over the priors. \n",
    "\n",
    "An approximate shortcut approach is to fix the priors at their most probable or expectation values. This method is known as empirical Bayes. \n",
    "\n",
    "Note that the posterior estimates of true velocities are biased relative to ML estimates,\n",
    "and have _smaller_ uncertainies (hence the name shrinkage) than their measurement uncertainties!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ab79a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "Recently, HB modeling has become an increasingly popular subfield of astrostatistics. \n",
    "\n",
    "HB modeling in astronomy is often applied to a set of sets of measurements that have \n",
    "something in common, such as measurements of individual phenomena that come from the \n",
    "same population (c.f. the above radial velocity example).\n",
    "\n",
    "Good examples include measurements of faint sources that share the same background, of individual supernova luminosities\n",
    "drawn from a population of cosmological type Ia supernovae, estimates of spectral energy \n",
    "distribution of stars that are all behind the same dust layer with unknown extinction \n",
    "properties, and estimates of planetary orbital eccentricity distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28a6fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion\n",
    "\n",
    "Another very important application of HB modeling, that is becoming more common, is \n",
    "accounting for measurement errors, intrinsic variance across a population, and various \n",
    "astronomical selection effects; a rather complex modeling framework is discussed in detail \n",
    "in a landmark study by Kelly et al. 2007 (ApJ 665, 1489)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   15,
   28,
   36,
   51,
   60,
   64,
   74,
   87,
   98,
   106,
   112,
   149,
   159,
   175,
   190,
   204,
   219,
   232,
   247,
   262,
   276,
   291,
   297,
   315,
   351,
   440,
   453,
   468
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}