{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5cd34f",
   "metadata": {},
   "source": [
    "# Deep Learning: Classifying Astronomical Images \n",
    "\n",
    "<!---\n",
    "### January 6, 2020 \n",
    "\n",
    "\n",
    "[astroML workshop at the 235th Meeting of the American Astronomical Society](http://www.astroml.org/workshops/AAS235.html)\n",
    "--->\n",
    "[Andrew Connolly, University of Washington](http://faculty.washington.edu/ajc26/)\n",
    "\n",
    "Thanks to Hayden Smotherman, University of Washington for the example networks.\n",
    "\n",
    "\n",
    "In this notebook we work through a simple example for a Neural Network using TensorFlow and the Keras interface. Initially we will start with a vanila network with two hidden layers and then expand this to a convolutional neural network with drop out layers and batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b8c56",
   "metadata": {},
   "source": [
    "Deep learning, an extension of the neural networks that were\n",
    "popularized in the 1990s. The concepts are inspired\n",
    "by the structure and function of the brain. A neuron in the brain is a\n",
    "core computational unit that takes a series of inputs from branched\n",
    "extensions of the neuron called dendrites, operates on these inputs,\n",
    "and generates an output that is transmitted along an axon to one or\n",
    "more other neurons. In the context of a neural network a neuron, $j$, takes a set of inputs,\n",
    "$x_i$, applies a, typically non-linear, function to these inputs and\n",
    "generates an output value. Networks are then created by connecting\n",
    "multiple neurons or layers of neurons to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d6a98",
   "metadata": {},
   "source": [
    "![Neural Network Diagram](figures/fig_neural_network-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4c47a",
   "metadata": {},
   "source": [
    "If we consider the simplified network\n",
    "inputs are passed to the neurons in the\n",
    "network. Each input is weighted by a value, $w_{ij}$ and the sum of\n",
    "these weighted inputs is operated on by a response or activation\n",
    "function $f(\\theta)$, which transform\n",
    "the input signal so that it varies between 0 and 1 through the\n",
    "application of a non-linear response. The output from any neuron is\n",
    "then given by,\n",
    "\n",
    "$$\n",
    "a_j =  f  \\left( \\sum_i w_{ij} x_i + b_j \\right)\n",
    "$$\n",
    "\n",
    "where $b_j$ is a bias term which determines the input level at which the\n",
    "neuron becomes activated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc8a650",
   "metadata": {},
   "source": [
    "We refer to the neurons between the inputs and the output layers as\n",
    "the hidden layers. If the neurons from one layer connect to all\n",
    "neurons in a subsequent layer we call this a fully connected layer.\n",
    "When the outputs from the neurons only connect to subsequent layers\n",
    "(i.e. the graph is acyclic) we refer to this as a feed-forward\n",
    "network -- this is the most common\n",
    "structure for a neural network used in classification. \n",
    "\n",
    "The final layer in the network is the output layer. As with the hidden\n",
    "layer, an activation function, $g(\\theta)$, in the output layer acts\n",
    "on the weighted sum of its inputs.  In this figure we have a single\n",
    "output node but there can be multiple outputs. For our example  network the\n",
    "output from the final neuron, $y_k$, would be given by\n",
    "\n",
    "$$\n",
    "y_k = g \\left( \\sum_j   w_{jk} a_j  + b_k \\right)  = g\\left( \\sum_j\n",
    "  w_{jk}  f \\left( \\sum_i w_{ij} x_i + b_j\\right) + b_k\\right)\n",
    "$$\n",
    "\n",
    "**Training of the network is simply the learning of the weights and bias values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a74e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to reduce the deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb18697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check we have a GPU available\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1716c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astroML.utils import split_samples\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2f76b",
   "metadata": {},
   "source": [
    "### Loading some visualization and helper functions\n",
    "\n",
    "These functions  ```normalize_image```, ```plot_image_array```, ```plot_confusion_matrix```, ```plot_model_history``` will be used to visualize the data and the outputs of the neural networks as a function of the type and complexity of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99404905",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def normalize_image(image):\n",
    "    '''Rescale the constrast in an image based on the noise (used for displays and the CNN)'''\n",
    "    sigmaG_coeff =  0.7413\n",
    "    image = image.reshape(21,21)\n",
    "    \n",
    "    per25,per50,per75 = np.percentile(image,[25,50,75])\n",
    "    sigmaG = sigmaG_coeff * (per75 - per25)\n",
    "    # sigma clip image, remove background, and normalize to unity\n",
    "    image[image<(per50-2*sigmaG)] = per50-2*sigmaG\n",
    "    image -= np.min(image)\n",
    "    image /= np.sum(image)\n",
    "    \n",
    "    return image\n",
    "    \n",
    "def plot_image_array(images, nrows=2, ncols=5, figsize=[8,4], nx=21, ny=21, title='', subtitle=False, \n",
    "                     class_true=None, classes=None):\n",
    "    '''Plot an array of images'''\n",
    "    fig, ax = plt.subplots(nrows=nrows,ncols=ncols,figsize=figsize)\n",
    "    fig.subplots_adjust(hspace=0, left=0.07, right=0.95, wspace=0.05, bottom=0.15)\n",
    "    for indx in np.arange(nrows*ncols):\n",
    "        i = int(indx/ncols)\n",
    "        j = indx%ncols\n",
    "        if (i == 0):\n",
    "            ax[i][j].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        if (j != 0):\n",
    "            ax[i][j].yaxis.set_major_formatter(plt.NullFormatter())\n",
    "\n",
    "        ax[i][j].imshow(images[indx].reshape(nx,ny), cmap='gray')\n",
    "        if (subtitle == True):\n",
    "            ax[i][j].set_title('True Class: %i, Predicted Class: %i\\n  Prob Class 1 %e ' % \n",
    "              (np.argmax(class_true[indx]), np.argmax(classes[indx]), classes[indx,1]))\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    ax[0][0].set_ylabel('$y$')\n",
    "    ax[nrows-1][int(ncols/2)].set_xlabel('$x$')            \n",
    "    \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, \n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    From scikit-learn: plots a confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    #fixes \"squishing of plot\"\n",
    "    plt.ylim([1.5, -.5]) \n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "def plot_model_history(history):\n",
    "    '''Plot the training and validation history for a TensorFlow network'''\n",
    "\n",
    "    # Extract loss and accuracy\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(10,5))\n",
    "    ax[0].plot(np.arange(n_epochs), loss, label='Training Loss')\n",
    "    ax[0].plot(np.arange(n_epochs), val_loss, label='Validation Loss')\n",
    "    ax[0].set_title('Loss Curves')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "\n",
    "    ax[1].plot(np.arange(n_epochs), acc, label='Training Accuracy')\n",
    "    ax[1].plot(np.arange(n_epochs), val_acc, label='Validation Accuracy')\n",
    "    ax[1].set_title('Accuracy Curves')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281c984",
   "metadata": {},
   "source": [
    "### Load the training samples and create test and validation data sets\n",
    "\n",
    "The data we are using is taken from a survey for NEOs by Lori Allen and collaborators using DECam on the Blanco 4m Telescope at CTIO. The data comprise a stack of images taken over a period of 5 nights. Within these images we search for slowly moving sources (TNOs) along potential orbital trajectories. Given these trajectories we coadd the images. Our goal is to determine whether there is a point source within the coadded images. The training sample includes images of simulated TNOs (true positives; stamps_sources.npz) and random trajectories where there is no known source (false positives; stamps_noise.npz). The true positives range in signal-to-noise from 100 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfaf6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brute force direct downloads source and noise images to circumvent size limitations \n",
    "# for google drive internal virus scan. Download may take some time.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "files = {'sources': (os.path.join('data', 'stamps_noise.npy'), '1UT2BCf-IDUEpvTmcU4bq6nDcY3Ayw5vJ'),\n",
    "         'noise': (os.path.join('data', 'stamps_sources.npy'), '1cZaMCA0z_nPX6GB_meLGouwOidEROcwc')}\n",
    "\n",
    "for name, file_id in files.values():\n",
    "    if not os.path.exists(name):\n",
    "        print(f\"Downloading file {name}.\")\n",
    "        \n",
    "        os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "        url = f\"https://docs.google.com/uc?export=download&id={file_id}&confirm=t\"\n",
    "        response = requests.post(url)\n",
    "        with open(name, 'wb') as file:\n",
    "            file.write(response.content) \n",
    "    print(f\"File {name} is downloaded\")\n",
    "            \n",
    "sources = np.load(files['sources'][0])\n",
    "noise = np.load(files['noise'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing images\n",
    "\n",
    "point_source_stamps = []\n",
    "for image in sources:\n",
    "    point_source_stamps.append(normalize_image(image))\n",
    "\n",
    "no_point_source_stamps = []\n",
    "for image in noise:\n",
    "    no_point_source_stamps.append(normalize_image(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2cf509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sample of images\n",
    "plot_image_array(no_point_source_stamps, title='false positives')\n",
    "plot_image_array(point_source_stamps, title='true positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003d3b7",
   "metadata": {},
   "source": [
    "### Create a training, validation, and test sample\n",
    "\n",
    "We will use astroML's split_samples to do this ```split_samples(input_stamps, stamp_class, [0.7,0.1,0.2])``` breaks the data in to random selections with  appropriate fractions of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "def reshape_arrays(data, labels):\n",
    "    '''reshape arrays for Keras'''\n",
    "    data = data.reshape(-1,21, 21, 1) \n",
    "    labels = to_categorical(labels)\n",
    "    return data,labels\n",
    "\n",
    "# combine the false positives and true positives\n",
    "input_stamps = np.vstack([no_point_source_stamps, point_source_stamps])\n",
    "stamp_class = np.zeros(len(no_point_source_stamps) + len(point_source_stamps))\n",
    "stamp_class[len(no_point_source_stamps):] = 1 # 0 for noise, 1 for a star\n",
    "\n",
    "# split the samples into training, validation and test data sets\n",
    "(data_train, data_val, data_test), (class_train, class_val, class_test) = split_samples(input_stamps, stamp_class, \n",
    "                                                                                        [0.7,0.1,0.2])\n",
    "data_train, class_train = reshape_arrays(data_train, class_train)\n",
    "data_val, class_val = reshape_arrays(data_val, class_val)\n",
    "data_test, class_test = reshape_arrays(data_test, class_test)\n",
    "\n",
    "print ('Number of samples in the training ({}); test ({}); and validation ({}) data sets'.format(data_train.shape[0], \n",
    "                                                                                    data_test.shape[0],\n",
    "                                                                                   data_val.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa2199",
   "metadata": {},
   "source": [
    "## Neural Network Frameworks\n",
    "\n",
    "\n",
    "The development and release of open source deep learning libraries has\n",
    "made the use of deep neural networks accessible to a wide range of\n",
    "fields. Currently there are two common packages PyTorch (https://pytorch.org) and Tensorflow (https://www.tensorflow.org). Either code base can be utilized for the\n",
    "figures and problems in this book (and generally they have the same\n",
    "functionality). \n",
    "\n",
    "### TensorFlow:\n",
    "\n",
    "Tensorflow is the more established code base with a large community\n",
    "and a large number of tutorials (https://www.tensorflow.org/tutorials) and online courses. Its\n",
    "functionality is more developed than PyTorch with tools to visualize\n",
    "and inspect a network (e.g., see TensorBoard). On the other hand, the\n",
    "learning curve for PyTorch is generally considered to be easier than\n",
    "that for Tensorflow with PyTorch having a more natural object oriented\n",
    "interface for people used to writing Python code.\n",
    "\n",
    "### PyTorch:\n",
    "\n",
    "The primary difference between TensorFlow and PyTorch is that the\n",
    "networks (or graphs) that TensorFlow generates are static while the\n",
    "networks for PyTorch are dynamic (see TensorFlow Fold for dynamic graphs). This means that with PyTorch one can\n",
    "modify and adjust the network on-the-fly (e.g., making it easier to\n",
    "adjust for changes in the input dimensionality or number of input\n",
    "nodes within a network). This feature and the object-oriented design of\n",
    "PyTorch often results in fewer lines of code to achieve the same\n",
    "solution when compared to Tensorflow.\n",
    "\n",
    "### Keras:  \n",
    "\n",
    "Keras is a high-level API written on top of TensorFlow (and its precursor Theano). It is written in Python and provides a simple and intuitive interface when building neural networks. It is currently released as part of TensorFlow.\n",
    "\n",
    "**What should you choose?** Both frameworks are continuously evolving.\n",
    "The choice of deep learning library will\n",
    "likely come down to which one you find better fits your style of\n",
    "programming and learning. For this tutorial we will use Keras as it has an intuitive implementation of the graphical or network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94faa54",
   "metadata": {},
   "source": [
    "### Building a network:\n",
    "\n",
    "Let's start by defining what we need for the network. We will start with Keras and\n",
    "\n",
    "- create a sequential model (this means we add layers one-by-one as we see in our introductory figure)\n",
    "- add a dense (fully connected) layer with 30 neurons \n",
    "  - **input_shape** describes the dimensionality of the _input data_ to this first hidden layer\n",
    "  - **activation** describes the activation fuction for the neurons (in this case we will be using 'relu'; rectified linear unit)\n",
    "- add a second dense (fully connected) layer with 30 neurons\n",
    "- flatten the output of the second layer into a single vector so we can use ```categorical_crossentropy``` as we are assuming that our classes are \"one-hot encoding\" (i.e. [1,0] or [0,1]\n",
    "- add an output layer using \"softmax\" (this means the activation values for each class sum to 1 so they can be treated like probabilities) with 2 nodes (_for our example we could have used a single output_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a199b6",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "Training a neural network is conceptually simple. Given a labelled set\n",
    "of data and a loss function, we need to optimize\n",
    "the weights and biases within the network by minimizing the loss.  A solution for training large\n",
    "networks\n",
    "uses backpropagation to efficiently estimate the gradient of the loss\n",
    "function with respect\n",
    "to the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ac28bc",
   "metadata": {},
   "source": [
    "## Creating a Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6752167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def simple(input_shape=(21, 21, 1), n_classes: int = 2):\n",
    "\n",
    "    model = tf.keras.models.Sequential(name='simple')\n",
    "\n",
    "    # input: 21x21 images with 1 channel -> (21, 21, 1) tensors.\n",
    "    model.add(tf.keras.layers.Dense(30, input_shape=input_shape, activation='relu', name='fc_1'))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(30, activation='relu', name='fc_2'))\n",
    "\n",
    "    # output layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation=activation, name='fc_out'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output the structure of the model\n",
    "simple_model = simple()\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca40908",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=1\n",
    "batch_size=1\n",
    "simple_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "simple_model_history = simple_model.fit(data_train, class_train, epochs=n_epochs, batch_size=batch_size,  verbose=1, \n",
    "                                        validation_data=(data_val, class_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31b99b",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "**Mini-batch:** Optimization of the weights uses a standard gradient descent technique. If the loss function can be expressed in terms of a sum over subsets of the training data (e.g., as is the case for the\n",
    "L2 norm) the training can be undertaken either for the dataset as a\n",
    "whole, for subsets of the data (batch learning), or for individual\n",
    "entries (on-line or stochastic learning). _Batch gradient descent_ looks at all points in the data and calculates the average gradients before updating the weights in the model. _Stochastic gradient descent_ takes a single point and calculates the gradients and then updates the model (and then repeats). _Mini-batch gradient descent_ takes a subset of the training data and  calculates the average gradients and  updates the model  (and then repeats over all mini-batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3228ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=20\n",
    "batch_size=\n",
    "simple_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "simple_model_history = simple_model.fit(data_train, class_train, epochs=n_epochs, batch_size=batch_size,  verbose=1, \n",
    "                                        validation_data=(data_val, class_val), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b0096b",
   "metadata": {},
   "source": [
    "### Performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the network to predict class values\n",
    "classes = simple_model.predict(data_test)\n",
    "print (classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703d34e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(np.argmax(class_test,axis=1), np.argmax(classes,axis=1), normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25239b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history of the network\n",
    "plot_model_history(simple_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5468c",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    "Our first optimization over the vanila NN. Batch normalization scales the activations from a layer (note we normalized the input data) to have zero mean and unit variance. In reality, the two parameters gamma (for the standard deviation) and beta (for the mean) are learned by the network and the activations multiplied/added by these parameters. Batch normalization provides a degree of regularization and allows for faster learning rates as the outputs are constrained to 0-1 (i.e. you dont get large excursions in the weights of subsequent layers in a network that need to be reoptimized/trained). \n",
    "\n",
    "The normalization is applied to mini-batches of training data (as opposed to using the full training sample)\n",
    "- add a batch normalization layer:  ```model.add(tf.keras.layers.BatchNormalization(axis = 3, name = 'bn_1'))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleBN(input_shape=(21, 21, 1), n_classes: int = 2):\n",
    "\n",
    "    model = tf.keras.models.Sequential(name='simple')\n",
    "\n",
    "    # input: 21x21 images with 1 channel -> (21, 21, 1) tensors.\n",
    "    model.add(tf.keras.layers.Dense(30, input_shape=input_shape, activation='relu', name='fc_1'))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(30, activation='relu', name='fc_2'))\n",
    "\n",
    "    # output layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation=activation, name='fc_out'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=20\n",
    "simple_model = simpleBN()\n",
    "simple_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "simple_model_history = simple_model.fit(data_train, class_train, epochs=n_epochs, batch_size=256, verbose=1, \n",
    "                                        validation_data=(data_val, class_val), shuffle=True)\n",
    "classes = simple_model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(np.argmax(class_test,axis=1), np.argmax(classes,axis=1), normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1324a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_history(simple_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ec54d",
   "metadata": {},
   "source": [
    "### Convolutional Networks\n",
    "\n",
    "\n",
    "Convolutional Neural Networks or\n",
    "CNNs are networks designed to work with images or with any regularly\n",
    "sampled dataset. CNNs reduce the complexity of the network by\n",
    "requiring that neurons only respond to inputs from a subset of an\n",
    "image (the receptive field). This mimics the operation of the visual\n",
    "cortex where neurons only respond to a small part of the\n",
    "field-of-view.\n",
    "\n",
    "There are four principal components to a CNN:\n",
    "- a convolutional layer,\n",
    "- a _non-linear activation function_ ,\n",
    "- a pooling or downsampling operation, and\n",
    "- a _fully connected layer for classification_\n",
    "\n",
    "Dependent on the complexity of the network or structure of the data,\n",
    "these components can occur singularly or chained together in multiple\n",
    "sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406fde9",
   "metadata": {},
   "source": [
    "![Convolutional Neural Network](figures/fig_cnn_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37803f8a",
   "metadata": {},
   "source": [
    "**Convolution** in a CNN refers to the convolution  of the input data\n",
    "$I(x,y)$ with a kernel $K(x,y)$ which will produce a feature map $F(x,y)$\n",
    "\n",
    "\\begin{equation}\n",
    "F(x,y) = K(x,y) * I(x,y)  = \\sum_{x_0} \\sum_{y_0} I(x-x_0, y-y_0) K(x_0, y_0).\n",
    "\\end{equation}\n",
    "\n",
    "The kernel only responds to pixels within its receptive field (i.e.,\n",
    "the size of the kernel), reducing the computational complexity of the\n",
    "resulting network. The kernels in the convolution are described by a\n",
    "depth (the number of kernels, $K$, applied to the image), and a stride\n",
    "(how many pixels a kernel shifts at each step in the convolution;\n",
    "typically one).  Given an $N\\times M$ image, the result of the\n",
    "convolution step is to transform a single image into a data cube of\n",
    "feature maps with a dimension $N \\times M \\times K$.\n",
    "\n",
    "Once **learned** the kernels within the convolutional layer can appear\n",
    "as physically intuitive operations on the images \n",
    "such as edge detection filters.\n",
    "\n",
    "\n",
    "As with traditional neural networks, a non-linear activation function\n",
    "is applied to the individual pixels in the resulting feature\n",
    "maps. \n",
    "\n",
    "\n",
    "The **pooling** in the CNN downsamples or subsamples the feature maps. Pooling summarizes values within a region\n",
    "of interest (e.g., a 2x2 pixel window). The summary can be the average\n",
    "pixel value but more commonly the maximum pixel value is preserved\n",
    "(Max Pooling) in the downsampling. This pooling of the feature maps\n",
    "reduces the size of the resulting network and makes the network less\n",
    "sensitive to small translations or distortions between images.\n",
    "\n",
    "\n",
    "\n",
    "The final layer of a CNN is the classification layer which maps the\n",
    "output of the CNN to a set of labels. This is typically a fully\n",
    "connected layer where each output\n",
    "of the final pooling layer connects to all neurons in the\n",
    "classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542f74f",
   "metadata": {},
   "source": [
    "### CNNs: Increasing the complexity of the architecture with VGG6\n",
    "\n",
    "Let's start with a simple network architecture from Oxford's Visual Geometry Group. VGG6 is a very simple network that performs well in traditional image classification competitions (e.g. those using the ImageNet data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def vgg6(input_shape=(21, 21, 1), n_classes: int = 2):\n",
    "    \"\"\"\n",
    "        VGG6\n",
    "    :param input_shape:\n",
    "    :param n_classes:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.models.Sequential(name='VGG6')\n",
    "    # input: 21x21 images with 1 channel -> (21, 21, 1) tensors.\n",
    "    # this applies 16 convolution filters of size 3x3 each.\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=input_shape, name='conv1'))\n",
    "    model.add(tf.keras.layers.Conv2D(16, (3, 3), activation='relu', name='conv2'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv3'))\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu', name='conv4'))\n",
    "    model.add(tf.keras.layers.BatchNormalization(axis = 3, name = 'bn_2'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(4, 4)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', name='fc_1'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    # output layer\n",
    "    activation = 'sigmoid' if n_classes == 1 else 'softmax'\n",
    "    model.add(tf.keras.layers.Dense(n_classes, activation=activation, name='fc_out'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7ca54",
   "metadata": {},
   "source": [
    "### The use of dropout layers\n",
    "\n",
    "As we increase the complexity of the network we run into the issue of overfitting the data (as seen in many of the astroML examples). The dropout layer ```model.add(tf.keras.layers.Dropout(0.5))``` at each training epoch randomly sets a neuron to 0 with a probability of 0.5. There is debate over whether the dropout layer should come before or after an activation layer but a recommended rule of thumb is that it should come after the activation layer for activation functions other than relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = vgg6()\n",
    "vgg_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "n_epochs=15\n",
    "vgg_model.summary()\n",
    "vgg_model_history = vgg_model.fit(data_train, class_train, epochs=n_epochs, batch_size=1024, verbose=2, \n",
    "                                  validation_data=(data_val, class_val), shuffle=True)\n",
    "classes = vgg_model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(np.argmax(class_test,axis=1), np.argmax(classes,axis=1), normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff70cb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plot_model_history(vgg_model_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682a9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example classifications\n",
    "plot_image_array(data_val, figsize=[16,10], subtitle=True, classes=classes, class_true=class_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b4f25",
   "metadata": {},
   "source": [
    "### Interpreting networks: how many layers and how many neurons?\n",
    "\n",
    "The number of layers, number of neurons in a layer, and the\n",
    "connectivity of these layers is typically described as the network\n",
    "architecture.  \n",
    "\n",
    "\n",
    "Approaches to defining a network\n",
    "architecture become more trial and error than applying an underlying\n",
    "set of principles. For a starting point, however, there are relatively\n",
    "few problems that benefit significantly from more than two layers and\n",
    "we recommend starting with a single layer when training an initial\n",
    "network and using cross-validation to determine when additional layers\n",
    "lead result in the data being overfit.\n",
    "\n",
    "As with the number of layers, the number of neurons within a layer\n",
    "drives the computational cost (and requiring progressively larger\n",
    "training sets to avoid overfitting of the data). There are many\n",
    "proposals for rules of thumb for defining a network architecture:\n",
    "- the number of neurons should lie between the number of inputs and output nodes\n",
    "- the number of neurons should be equal to the number of outputs plus 2/3rd the number input nodes.\n",
    "- the number of neurons in the hidden layer should be less than twice the size of the input layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5525ff0e",
   "metadata": {},
   "source": [
    "### Interpreting networks: where is a network looking\n",
    "\n",
    "Occulsion maps, saliency maps, class activation maps are all techniques for expressing which pixels contribute to classification. These are attempts to reduce the \"black box\" nature of the networks. The simplest of these is the occlussion map where we part of an image and calculate the probability of it belonging to a class. If the probability decreases the occluded part of the image is assumed to be important. If there is no change in probability the occluded pixels are not assumed to be important. A simple implementation of this is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5856b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vgg_model\n",
    "image_number = 11\n",
    "\n",
    "kernel_size=5\n",
    "input_stamp = data_test[image_number].reshape(21,21)\n",
    "i = 0\n",
    "j=0\n",
    "heatmap = []\n",
    "keras_stamps = []\n",
    "for j in range(22-kernel_size):\n",
    "    for i in range(22-kernel_size):\n",
    "        img = np.copy(input_stamp)\n",
    "        img[i:i+kernel_size,j:j+kernel_size] = 0\n",
    "        img = normalize_image(img)\n",
    "        keras_stamps.append(img)\n",
    "keras_stamps = np.array(keras_stamps).reshape([-1,21,21,1])\n",
    "probs = 1. - model.predict(keras_stamps)\n",
    "heatmap = probs[:,1].reshape(22-kernel_size,22-kernel_size)\n",
    "\n",
    "def transparent_cmap(cmap, N=255):\n",
    "    \"Copy colormap and set alpha values\"\n",
    "    mycmap = cmap\n",
    "    mycmap._init()\n",
    "    mycmap._lut[:,-1] = np.linspace(0, 0.8, N+4)\n",
    "    return mycmap\n",
    "\n",
    "# pad heatmap to same size as original image\n",
    "heatmap = np.pad(heatmap, pad_width=np.int(kernel_size/2), mode='minimum')\n",
    "\n",
    "# use the base cmap to create transparent overlay\n",
    "mycmap = transparent_cmap(plt.cm.Reds)\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1)\n",
    "ax.imshow(data_test[image_number].reshape(21,21), cmap='gray')\n",
    "ax.imshow(np.array(heatmap), alpha=0.5, cmap=mycmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232cc5d",
   "metadata": {},
   "source": [
    "### Exercise for the reader: more complicated architectures: resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/priya-dwivedi/Deep-Learning/blob/master/resnet_keras/Residual_Networks_yourself.ipynb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "def identity_block(X, f, filters, stage, block):\n",
    "    \"\"\"\n",
    "    Implementation of the identity block as defined in Figure 3\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    \n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, f, filters, stage, block, s = 2):\n",
    "    \"\"\"\n",
    "    Implementation of the convolutional block as defined in Figure 4\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window for the main path\n",
    "    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n",
    "    stage -- integer, used to name the layers, depending on their position in the network\n",
    "    block -- string/character, used to name the layers, depending on their position in the network\n",
    "    s -- Integer, specifying the stride to be used\n",
    "    \n",
    "    Returns:\n",
    "    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    X_shortcut = X\n",
    "\n",
    "\n",
    "    ##### MAIN PATH #####\n",
    "    # First component of main path \n",
    "    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "\n",
    "    # Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "\n",
    "    ##### SHORTCUT PATH #### (≈2 lines)\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s), padding = 'valid', name = conv_name_base + '1',\n",
    "                        kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    \n",
    "    return X\n",
    "\n",
    "def ResNet50(input_shape=(21, 21, 1), classes=2):\n",
    "    \"\"\"\n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
    "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> TOPLAYER\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the images of the dataset\n",
    "    classes -- integer, number of classes\n",
    "\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "\n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
    "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], stage = 3, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
    "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
    "\n",
    "    # Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], stage = 4, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
    "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
    "\n",
    "    # Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], stage = 5, block='a', s = 2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    # AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    #X = AveragePooling2D((2,2), name=\"avg_pool\")(X)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_model = ResNet50()\n",
    "resnet50_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "n_epochs=5\n",
    "resnet_model_history = resnet50_model.fit(data_train, class_train, epochs=n_epochs, batch_size=256, verbose=1, \n",
    "                                          validation_data=(data_val, class_val), shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   13,
   31,
   44,
   48,
   66,
   89,
   95,
   101,
   106,
   112,
   225,
   231,
   256,
   268,
   272,
   278,
   301,
   342,
   356,
   367,
   371,
   394,
   400,
   406,
   415,
   421,
   425,
   431,
   437,
   440,
   449,
   467,
   476,
   482,
   484,
   507,
   511,
   554,
   560,
   597,
   603,
   613,
   619,
   624,
   627,
   652,
   658,
   693,
   697,
   876
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}